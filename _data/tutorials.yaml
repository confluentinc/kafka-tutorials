transforming:
  title: "How to transform a stream of events"
  meta-description: "Learn to transform a stream of events"
  slug: "/transform-a-stream-of-events"
  question: "How do I transform a field in a stream of events in a Kafka topic?"
  introduction: "Consider a topic with events that represent movies. Each event has a single attribute that combines its title and its release year into a string. In this tutorial, we'll write a program that creates a new topic with the title and release date turned into their own attributes."
  status:
    ksql: enabled
    kstreams: enabled
    kafka: enabled

filtering:
  title: "How to filter a stream of events"
  meta-description: "Learn to filter a stream of events"
  slug: "/filter-a-stream-of-events"
  question: "How do I filter messages in a Kafka topic to contain only those that I'm interested in?"
  introduction: "Consider a topic with events that represent book publications. In this tutorial, we'll write a program that creates a new topic which only contains the events for a particular author."
  status:
    ksql: enabled
    kstreams: enabled
    kafka: enabled

splitting:
  title: "How to split a stream of events into substreams"
  meta-description: "Learn to split a stream of events into substreams"
  slug: "/split-a-stream-of-events-into-substreams"
  question: "How do I split events in a Kafka topic so that the events are placed into subtopics?"
  introduction: "Suppose that you have a Kafka topic representing appearances of an actor or actress in a film, with each event denoting the genre. In this tutorial, we'll write a program that splits the stream into substreams based on the genre. We'll have a topic for drama films, a topic for fantasy films, and a topic for everything else."
  status:
    ksql: enabled
    kstreams: enabled
    kafka: enabled

merging:
  title: "How to merge many streams into one stream"
  meta-description: "Learn to merge many streams into one stream"
  slug: "/merge-many-streams-into-one-stream"
  question: "If I have many Kafka topics with events, how do I merge them all into a single topic?"
  introduction: "Suppose that you have a set of Kafka topics representing songs being played of a particular genre. You might have a topic for rock songs, another for classical songs, and so forth. In this tutorial, we'll write a program that merges all of the song play events into a single topic."
  status:
    ksql: enabled
    kstreams: enabled
    kafka: enabled

joining-stream-table:
  title: "How to join a stream and a lookup table"
  meta-description: "Learn to join a stream and a lookup table"
  slug: "/join-a-stream-to-a-table"
  question: "If I have events in a Kafka topic and a table of reference data (aka a lookup table), how can I join each event in the stream to a piece of data in the table based on a common key?"
  introduction: "Suppose you have a set of movies that have been released and a stream of ratings from movie-goers about how entertaining they are. In this tutorial, we'll write a program that joins each rating with content about the movie."
  status:
    ksql: enabled
    kstreams: enabled

joining-table-table:
  title: "How to join a table and a table"
  meta-description: "Learn to join a table and a table"
  slug: "/join-a-table-to-a-table"
  question: "If I have two tables of reference data in Kafka topics, how can I join the tables on a common key?"
  introduction: "Suppose you have a set of data about movies and want to add further details such as who the lead actor was. In this tutorial, we'll write a program that joins each movie to another set of data holding information about who the lead actor was for movies."
  status:
    ksql: enabled
    kstreams: enabled

joining-stream-stream:
  title: "How to join a stream and a stream"
  meta-description: "Learn to join a stream and a stream"
  slug: "/join-a-stream-to-a-stream"
  question: "If I have event streams in two Kafka topics, how can I join them together and create a new topic based on a common identifying attribute, where the new events are enriched from the original topics?"
  introduction: "Suppose you have two streams containing events for orders and shipments. In this tutorial, we'll write a program that joins these two streams to create a new, enriched one. The new stream will tell us which orders have been successfully shipped, how long it took for them to ship, and which warehouse they shipped from."
  status:
    ksql: enabled
    kstreams: enabled

fk-joins:
  title: "How to join a table and a table with a foreign key"
  meta-description: "Learn how to join two tables with different primary keys"
  slug: "/foreign-key-joins"
  question: "How can I join two tables with different primary keys?"
  introduction: "Suppose you are running an internet streaming music service where you offer albums or individual music tracks for sale.  You'd like to track trends in listener preference by joining the track purchases against the table of albums.  The issue is that the track purchase key doesn't align with the primary key for the album table.  However, since the value of the track purchase contains the id of the album, you can extract the album id from the track purchase and complete a foreign key join against the album table."
  status:
    kstreams: enabled

rekeying:
  title: "How to rekey a stream with a value"
  meta-description: "Learn to rekey a stream with a value"
  slug: "/rekey-a-stream"
  question: "How can I add a key or change the key to a Kafka topic?"
  introduction: "Suppose you have an unkeyed stream of movie ratings from movie-goers. Because the stream is not keyed, ratings for the same movie aren't guaranteed to be placed into the same partition. In this tutorial, we'll write a program that creates a new topic keyed by the movie's name. When the key is consistent, it becomes possible to process these ratings at scale and in parallel."
  status:
    ksql: enabled
    kstreams: enabled

rekeying-function:
  title: "How to rekey a stream with a function"
  meta-description: "Learn to rekey a stream with a function"
  slug: "/rekey-with-function"
  question: "How can I rekey records in a Kafka topic, whereby the new key is a variation of data currently in the payload?"
  introduction: "Consider a stream of customer information events keyed by id. Each event contains a few attributes, including the customer's phone number. In this tutorial, we'll write a program that rekeys the topic by the area code of the phone number. Customers of the same area code will be placed into the same partition in the new topic."
  status:
    ksql: enabled
    kstreams: enabled

tumbling-windows:
  title: "How to create tumbling windows"
  meta-description: "Learn to create tumbling windows"
  slug: "/create-tumbling-windows"
  question: "If I have time-series events in a Kafka topic, how can I group them into fixed-size, non-overlapping, contiguous time intervals?"
  introduction: "Suppose you have a topic with events that represent ratings of movies. In this tutorial, we'll write a program that maintains tumbling windows counting the total number of ratings that each movie has received."
  status:
    ksql: enabled
    kstreams: enabled

hopping-windows:
  title: "How to create hopping windows"
  meta-description: "Learn to create hopping windows"
  slug: "/create-hopping-windows"
  question: "If I have time-series events in a Kafka topic, how can I group them into fixed-size, possibly-overlapping, contiguous time intervals to identify a specific scenario?"
  introduction: "You want to build an alerting system that automatically detects if the temperature of a room consistently drops. In this tutorial, we'll write a program that monitors a stream of temperature readings and detects when the temperature consistently drops below 45 Â°F for a period of 10 minutes."
  status:
    ksql: enabled

session-windows:
  title: "Create session windows"
  slug: "/create-session-windows"
  question: "If I have time-series events in a Kafka topic, how can I group them into variable-size, non-overlapping time intervals based on a configurable inactivity period?"
  introduction: "Given a topic of click events on a website, there are various ways that we can process it. As well as simply counting the number of clicks in a regular time frame (using hopping or tumbling windows), we can also perform sessionization on the data. Here the length of the time window is based on the concept of a session, which is defined based on a period of inactivity. A given user might visit a website multiple times a day, but in distinct visits. Using session windows, we can analyze the number of clicks and duration per visit."
  status:
    ksql: enabled
    kstreams: enabled

aggregating-count:
  title: "How to count a stream of events"
  meta-description: "Learn to count a stream of events"
  slug: "/create-stateful-aggregation-count"
  question: "How can I count the number of events in a Kafka topic based on some criteria?"
  introduction: "Suppose you have a topic with events that represent ticket sales for movies. In this tutorial, you'll see an example of 'groupby count' in Kafka Streams and ksqlDB.  We'll write a program that calculates the total number of tickets sold per movie."
  status:
    ksql: enabled
    kstreams: enabled

aggregating-minmax:
  title: "How to find the min/max in a stream of events"
  meta-description: "Learn to find the min/max in a stream of events"
  slug: "/create-stateful-aggregation-minmax"
  question: "How can I get the minimum or maximum value of a field from all records in a Kafka topic?"
  introduction: "Suppose you have a topic with events that represent ticket sales of movies. In this tutorial, we'll write a program that calculates the maximum and minimum revenue of movies by year."
  status:
    ksql: enabled
    kstreams: enabled

aggregating-sum:
   title: "How to sum a stream of events"
   meta-description: "Learn to sum a stream of events"
   slug: "/create-stateful-aggregation-sum"
   question: "How can I calculate the sum of one or more fields form all records in a Kafka topic?"
   introduction: "Suppose you have a topic with events that represent ticket sales for movies. Each event contains the movie that the ticket was purchased for as well as its price. In this tutorial, we'll write a program that calculates the sum of all ticket sales per movie."
   status:
     ksql: enabled
     kstreams: enabled

serialization:
  title: "How to convert a stream's serialization format"
  meta-description: "Learn to convert a stream's serialization format"
  slug: "/changing-serialization-format"
  question: "If I have a Kafka topic with the data serialized in a particular format, how can I change the format to something else?"
  introduction: "Consider a topic with events that represent movie releases. The events in the topic are formatted with Avro. In this tutorial, we'll write a program that creates a new topic with the same events, but formatted with Protobuf."
  status:
    ksql: enabled
    kstreams: enabled

connect-add-key-to-source:
  title: "Add key to data ingested through Kafka Connect"
  slug: "/connect-add-key-to-source"
  question: "How can I stream data from a source system (such as a database) into Kafka using Kafka Connect, and add a key to the data as part of the ingest?"
  introduction: "Kafka Connect is the integration API for Apache Kafka. It enables you to stream data from source systems (such databases, message queues, SaaS platforms, and flat files) into Kafka, and from Kafka to target systems. When you stream data into Kafka you often need to set the key correctly for partitioning and application logic reasons. In this example there is data about cities in a database, and we want to key the resulting Kafka message by the city_id field. There are different ways to set the key correctly and these tutorials will show you how. It will also cover how to declare the schema and use Kafka Streams to process the data using SpecificAvro. "
  status:
    ksql: enabled
    kafka: enabled
    kstreams: enabled

finding-distinct:
  title: "How to find distinct values in a stream of events"
  slug: "/finding-distinct-events"
  question: "How can I filter out duplicate events from a Kafka topic based on a field in the event, producing a new stream of unique events per time window?"
  introduction: "Consider a topic with events that represent clicks on a website.  Each event contains an IP address, a URL, and a timestamp.  In this tutorial, we'll write a program that filters click events by the IP address within a window of time."
  status:
    ksql: enabled
    kstreams: enabled
    kafka: disabled

window-final-result:
  title: "Emit a final result from a time window"
  slug: "/window-final-result"
  question: "How can I count the number of messages in a Kafka topic, per key over a time window, getting a final result that takes into account late arrivals?"
  introduction: "Consider a topic with events that represent sensor warnings (pressure on robotic arms). One warning per time slot is fine, but you don't want to have too much warnings at the same time. In this tutorial, we'll write a program that counts the messages of a same sensor and sends a result at the end of the window."
  status:
    kstreams: enabled

udf:
  title: "How to build a User-Defined Function (UDF) to transform events"
  slug: "/udf"
  question: "How can I transform the values of a Kafka topic using a stateless scalar function not already provided by ksqlDB?"
  introduction: "Consider a topic of stock price events that you want to calculate the <a href=\"https://en.wikipedia.org/wiki/Volume-weighted_average_price\">volume-weighted average price</a> (VWAP) for each event, publishing the result to a new topic.  There is no built-in function for VWAP, so we'll write a custom <a href=\"https://docs.confluent.io/current/ksql/docs/developer-guide/udf.html\">KSQL UDF</a> that performs the calculation."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled

deserialization-errors:
  title: "How to handle deserialization errors"
  meta-description: "How to handle deserialization errors"
  slug: "/handling-deserialization-errors"
  question: "How can I identify and manage deserialization errors that cause some events from a Kafka topic to not be written into a stream or table?"
  introduction: "During the development of event streaming applications, it is very common to have situations where some streams or tables are not receiving some events that has been sent to it. Often this happens because there was an deserialization error due to the event not being in the right format, but that is not so trivial to figure out. In this tutorial, we'll write a program that monitors a stream of sensors. Any deserialization error that happens in this stream will be made available in another stream that can be queried to check errors."
  status:
    ksql: enabled

flatten-nested-data:
  title: "How to flatten deeply nested events"
  meta-description: "How to flatten deeply nested events"
  slug: "/flatten-nested-data"
  question: "How can I transform a stream of events with nested data into a flattened dataset that is simpler to handle?"
  introduction: "Consider a topic containing product orders. Each order contains data about the customer and the product, specified as nested data. In this tutorial, we'll write a program that transforms each order into a new version of it that contains all the data as flat fields."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled

generate-test-data-streams:
  title: "Generate complex streams of test data"
  meta-description: "Generate complex and realistic streams of test data"
  question: "How can I generate realistic test data in Kafka?"
  slug: "/generate-streams-of-test-data"
  introduction: "Perhaps you are building an application, or constructing a pipeline, and you would like some mock data to test it with. Using this connector it's possible to generate realistic test data that can also be made referentially consistent across topics."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: enabled

aggregating-average:
  title: "Compute an average aggregation"
  meta-description: "Learn how to compute an average aggregation like count or sum"
  slug: "/aggregating-average"
  question: "How can I implement an average aggregation that implements incremental functions, namely count and sum?"
  introduction: "Kafka Streams natively supports \"incremental\" aggregation functions, in which the aggregation result is updated based on the values captured by each window. Incremental functions include count, sum, min, and max. An average aggregation cannot be computed incrementally. However, as this tutorial shows, it can be implemented by composing incremental functions, namely count and sum. Consider a topic with events that represent ratings of movies. In this tutorial, we'll write a program that calculates and maintains a running average rating for each movie."
  status:
    ksql: disabled
    kstreams: enabled
    kafka: disabled

dynamic-output-topic:
  title: "How to dynamically choose the output topic"
  meta-description: "Learn how to dynamically route records to different Kafka topics at runtime"
  slug: "/dynamic-output-topic"
  question: "How can I dynamically route records to different Kafka topics?"
  introduction: "Consider a situation where, depending on data in your records, you need to direct output to different topics.  In this tutorial, you'll learn how to instruct Kafka Streams to choose the output topic at runtime."
  status:
    ksql: disabled
    kstreams: enabled
    kafka: disabled

naming-changelog-repartition-topics:
  title: "How to name stateful operations in Kafka Streams"
  meta-description: "How to name stateful operations in Kafka Streams"
  slug: "/naming-stateful-operations"
  question: "How can I change the topology of an existing Kafka Streams application that is compatible with the existing one?"
  introduction: "You want to add or remove some operations in your Kafka Streams application.  In this tutorial we'll name the changelog and repartition topics so that the topology updates don't break compatibility."
  status:
    ksql: disabled
    kstreams: enabled
    kafka: disabled

cogrouping-streams:
  title: "How to combine stream aggregates together in a single larger object"
  meta-description: "How to combine stream aggregates together in a single larger object"
  slug: "/cogrouping-streams"
  question: "How do I combine aggregate values like `count` from multiple streams into a single result?"
  introduction: "You want to compute the count of user login events per application in your system, grouping the individual result from each source stream into one aggregated object.  In this tutorial we'll cover how to use the Kafka Streams Cogrouping functionality to accomplish this task with clear, performant code"
  status:
    ksql: disabled
    kstreams: enabled
    kafka: disabled

console-consumer-producer-basic:
  title: "Console Producer and Consumer Basics, no (de)serializers"
  meta-description: "Produce and consume your first message with Kafka"
  slug: "/kafka-console-consumer-producer-basics"
  question: "What is the simplest way to write messages to and read messages from Kafka?"
  introduction: "So you are excited to get started with Kafka and you'd like to produce and consume some basic messages and you want to do so quickly.  In this tutorial we'll show you how to produce and consume messages from the command line with no code! Note this tutorial doesn't cover using (de)serializers. Using (de)serializers with the console consumer and producer are covered in separate tutorials."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

console-consumer-primitive-keys-values:
  title: "How to use the console consumer to read non-string primitive keys and values"
  meta-description: "How to use the console consumer to read non-string primitive keys and values"
  slug: "/console-consumer-primitive-values"
  question: "How do I specify key and value deserializers when running the Kafka console consumer?"
  introduction: "You want to inspect/debug records written to a topic.  Each record key and value is a long and double, respectively.  In this tutorial you'll learn how to specify key and value deserializers with the console consumer."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

console-consumer-read-specific-offsets-partition:
  title: "How to read from a specific offset and partition with the Kafka Console Consumer"
  meta-description: "How to read from a specific offset and partition with the Kafka Console Consumer"
  slug: "/kafka-console-consumer-read-specific-offsets-partitions"
  question: "How do I read from a specific offset and partition of a Kafka topic?"
  introduction: "You are confirming record arrivals and you'd like to read from a specific offset in a topic partition.   In this tutorial you'll learn how to use the Kafka console consumer to quickly debug issues by reading from a specific offset as well as control the number of records you read."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

kafka-connect-datagen-local:
  title: "How to generate mock data to a local Kafka topic using the Kafka Connect Datagen"
  meta-description: "How to generate mock data to a local Kafka topic using the Kafka Connect Datagen"
  slug: "/kafka-connect-datagen-local"
  question: "How can I produce mock data to Kafka topics to test my Kafka applications?"
  introduction: "You will run a local instance of the <a href=\"https://www.confluent.io/hub/confluentinc/kafka-connect-datagen\">Kafka Connect Datagen</a> connector to produce mock data to a local Kafka cluster. This facilitates learning about Kafka and testing your applications."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

kafka-connect-datagen-ccloud:
  title: "How to generate mock data to a Kafka topic in Confluent Cloud using the fully-managed Kafka Connect Datagen"
  meta-description: "How to generate mock data to a Kafka topic in Confluent Cloud using the fully-managed Kafka Connect Datagen"
  slug: "/kafka-connect-datagen-ccloud"
  question: "How can I produce mock data to Kafka topics in Confluent Cloud to test my Kafka applications?"
  introduction: "You will run a fully-managed instance of the <a href=\"https://www.confluent.io/hub/confluentinc/kafka-connect-datagen\">Kafka Connect Datagen</a> connector to produce mock data to a Kafka topic in <a href=\"https://www.confluent.io/confluent-cloud/tryfree/\">Confluent Cloud</a>. This helps you test your applications in the cloud."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

change-topic-partitions-replicas:
  title: "How to change the number of partitions and replicas of a Kafka topic"
  meta-description: "How to change the number of partitions or replicas of a Kafka topic"
  slug: "/change-topic-partitions-replicas"
  question: "How do I change the number of partitions or replicas of a Kafka topic?"
  introduction: "If you have a Kafka topic but want to change the number of partitions or replicas, you can use a streaming transformation to automatically stream all the messages from the original topic into a new Kafka topic which has the desired number of partitions or replicas."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled

kafka-consumer-application:
  title: "How to build your first Apache KafkaConsumer application"
  meta-description: "How build your first Apache Kafka consumer application"
  slug: "/creating-first-apache-kafka-consumer-application"
  question: "How do I get started in building my first Kafka consumer application?"
  introduction: "You'd like to integrate an Apache KafkaConsumer in your event-driven application, but you're not sure where to start.  In this tutorial you'll build a small application reading records from Kafka with a KafkaConsumer.  You can use the code in this tutorial as an example of how to use an Apache Kafka consumer."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

produce-consume-lang:
  title: "Produce and Consume Records in multiple languages"
  meta-description: "Produce and Consume events from Apache Kafka in multiple languages"
  slug: "/produce-and-consume-lang/"
  question: "How can I produce and consume events from Kafka using a programming language other than Java?"
  introduction: "You want to enrich and expose a list of books from a library. You have to produce an event for each book acquisition (title, editor, release ...), and consume back the same events to serve the book collection over HTTP."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    python: disabled
    ruby: disabled
    scala: enabled
    swift: disabled

streams-to-table:
  title: "How to convert a Kafka Streams KStream to a KTable"
  meta-description: "How to convert a Kafka Streams KStream to a KTable"
  slug: "/kafka-streams-convert-to-ktable"
  question: "How do I convert a KStream to a KTable without having to perform a dummy aggregation operation?"
  introduction: "You have a KStream and you need to convert it to a KTable, but you don't need an aggregation operation. With the 2.5 release of Apache Kafka, Kafka Streams introduced a new method KStream.toTable allowing users to easily convert a KStream to a KTable without having to perform an aggregation operation."
  status:
    ksql: disabled
    kstreams: enabled
    kafka: disabled

kafka-producer-application:
  title: "How build your first Apache KafkaProducer application"
  meta-description: "How build your first Apache Kafka producer application"
  slug: "/creating-first-apache-kafka-producer-application"
  question: "How do I get started in building my first Kafka producer application?"
  introduction: "You'd like to integrate an Apache KafkaProducer in your event-driven application, but you're not sure where to start.  In this tutorial you'll build a small application writing records to Kafka with a KafkaProducer.  You can use the code in this tutorial as an example of how to use an Apache Kafka producer"
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

kafka-producer-application-callback:
  title: "How build an Apache KafkaProducer application using callbacks"
  meta-description: "How build an Apache Kafka producer application and handle responses using the Callback interface"
  slug: "/kafka-producer-callback-application"
  question: "How do I use callbacks with a KafkaProducer to handle produce responses?"
  introduction: "You have an application using a Apache KafkaProducer, but you want to have an automatic way of handing the responses after producing records.  In this tutorial you learn how to use the Callback interface to automatically handle responses from producing records."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

console-consumer-producer-avro:
  title: "Console Producer and Consumer for Avro messages"
  meta-description: "Produce and consume your first message with Kafka in Avro format using Schema Registry"
  slug: "/kafka-console-consumer-producer-avro"
  question: "What is the simplest way to write and read messages encoded in Avro to and from Kafka?"
  introduction: "So you are excited to get started with Kafka and you would like to produce and consume some messages in Avro format.  In this tutorial we'll show you how to produce and consume messages from the command line with no code!"
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

count-messages:
  title: "How to count messages in a Kafka topic"
  meta-description: "Learn how to count the number of messages in a Kafka topic"
  slug: "/how-to-count-messages-on-a-kafka-topic"
  question: "How can I count the number of messages in a Kafka topic?"
  introduction: "Messages are stored in Kafka topics until they are removed by Kafka. It can be useful to know how many messages there are currently in a topic. In this example we'll take a topic of pageview data and see how we can count all of the messages in the topic."
  status:
    ksql: enabled

ccloud-produce-consume:
  title: "How to produce and consume records from Confluent Cloud using ccloud CLI"
  meta-description: "Learn how to produce and consume records from Confluent Cloud using ccloud CLI"
  slug: "/ccloud-produce-consume"
  question: "How do I use the ccloud CLI to produce and consume records from a Confluent Cloud topic?"
  introduction: "In this tutorial you learn how to use the ccloud CLI to produce and consume records from a Kafka topic in <a href=\"https://www.confluent.io/confluent-cloud/tryfree/\">Confluent Cloud</a>. This can help you build and debug your Confluent Cloud based event streaming applications."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled
    c : disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

kafka-streams-schedule-operations:
  title: "How to schedule operations in Kafka Streams"
  meta-description: "Learn how to schedule operations in Kafka Streams that execute at regular intervals"
  slug: "/kafka-streams-schedule-operations"
  question: "How can I schedule recurring operations in Kafka Streams?"
  introduction: "You'd like to have some periodic functionality execute in your Kafka Streams application.  In this tutorial you'll learn how to use puncuation in Kafka Streams for executing work at regular intervals."
  status:
    ksql: disabled
    kstreams: enabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

message-ordering:
  title: "How to maintain message ordering and no message duplication"
  meta-description: "How to maintain message order and prevent duplication in a Kafka topic partition using the idempotent producer"
  slug: "/message-ordering"
  question: "How can I maintain the order of messages and prevent message duplication in a Kafka topic partition?"
  introduction: "If your application needs to maintain ordering of messages with no duplication, you can enable your Apache Kafka producer for idempotency. An idempotent producer has a unique producer ID and uses sequence IDs for each message, which allows the broker to ensure it is committing ordered messages with no duplication, on a per partition basis."
  status:
    ksql: disabled
    kstreams: disabled
    kafka: enabled

masking-data:
  title: "Masking data"
  meta-description: "Learn how to remove or obfuscate fields in Kafka records by masking them"
  slug: "/masking-data"
  question: "How can I mask fields in my Kafka topic?"
  introduction: "Suppose you have a topic that contains personally identifiable information (PII), we'll write a program to persist the events in the original topic to a new Kafka topic with the PII removed or obfuscated."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

anomaly-detection:
  title: "Anomaly detection"
  meta-description: "Learn how to detect anomalies in a stream of Kafka events"
  slug: "/anomaly-detection"
  question: "If I have time-series events in a Kafka topic, how do I find anomalous events?"
  introduction: "A common pattern for fraudsters is to disguise transactions under the name of a popular company, the idea being that the chances of them being recognized is very low. For example, transactions labeled Verizon, Citibank, USPS, etc., are likely to look similar and blend in.  This tutorial shows you how to identify this pattern of behavior by detecting 'abnormal' transactions that occur within a window of time

                 There will normally be a group of such transactions that occur within a 24-hour period. When it comes to fraud detection, financial institutions will categorize this behavior as unusual and alert their fraud team to investigate immediately. Other example use cases include detecting atm fraud or unusual credit card activity."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

time-concepts:
  title: "Event-time semantics"
  meta-description: "Learn how to extract the timestamp within the Kafka record payload to achieve event-time semantics"
  slug: "/time-concepts"
  question: "What is the difference between using the timestamp from the record metadata and using the timestamp within the record payload?"
  introduction: "By default, time-based aggregations in Kafka Streams and ksqlDB (<a href=\"https://kafka-tutorials.confluent.io/create-tumbling-windows/ksql.html\">tumbling windows</a>, <a href=\"https://kafka-tutorials.confluent.io/create-hopping-windows/ksql.html\">hopping windows</a>, etc.) operate on the timestamp in the record metadata, which could be either 'CreateTime' (producer system time) or 'LogAppendTime' (broker system time), depending on the <a href=\"https://docs.confluent.io/platform/current/installation/configuration/topic-configs.html#message.timestamp.type\">message.timestamp.type</a> configuration value. 'CreateTime' helps with event-time semantics, but there may be use cases where the desired event time is a timestamp embedded inside the record payload. This tutorial shows you how to use the timestamp either from the record metadata or from a field in the record payload."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled

ksql-nested-json:
  title: "Working with nested JSON"
  meta-description: "Learn how to select fields from a stream of Kafka records that are contained in nested JSON"
  slug: "/working-with-nested-json"
  question: "How can I select fields from a stream of records that are contained in deeply nested JSON?"
  introduction: "Suppose you have a topic with JSON-formatted records, but it contains nested objects.  You want to write a query that can access fields in those nested objects"
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

ksql-heterogeneous-json:
  title: "Working with heterogenous JSON records"
  meta-description: "Learn how to work with heterogenous JSON records that have different structures or different values"
  slug: "/working-with-json-different-structure"
  question: "How do I select fields from a stream of records with different structures and possibly different values?"
  introduction: "Suppose you have a topic with JSON-formatted records, but not all the records have the same structure and value types.  You want to write a query that can handle the different structures and pull out specific fields "
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

multi-joins:
  title: "Multi-join expressions"
  meta-description: "Learn how to collapse multiple joins into a single statement with ksqlDB"
  slug: "/multi-joins"
  question: "How can I join multiple streams or tables together using a single expression in ksqlDB?"
  introduction: "Suppose you have two tables (customers and items) and one stream (orders) that have purchases from an online store. You want to build a stream of all orders and customers who made purchases including item details."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

concatenation:
  title: "Concatenation"
  meta-description: "Concatenation"
  slug: "/concatenation"
  question: "How do I concatenate values from multiple columns into a single one?"
  introduction: "Suppose you have a table and you need to combine 2 or more columns into a single value.  In this tutorial we'll show how to use the concatenation operator to create a single value from multiple columns"
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

column-difference:
  title: "Column Difference"
  meta-description: "Column Difference"
  slug: "/column-difference"
  question: "How do I calculate the difference between two columns?"
  introduction: "Suppose you have a table or stream and you need to calculate the difference between two columns.  In this tutorial we'll show how to calculate the difference between two columns"
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

geo-distance:
  title: "Calculating lat-long distance"
  meta-description: "Calculating lat-long distance"
  slug: "/geo-distance"
  question: "How do I calculate the distance between two lat-long points?"
  introduction: "Suppose you work for a company that insures cellphones. The company records events that would result in an insurance claim, such as a customer dropping their phone in water. The company has data about where the event occurred and repair shop lat-long data. It's your job to recommend the closest repair shop to the customers in kilometers."
  status:
    ksql: enabled
    kstreams: disabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled

sliding-windows:
  title: "How to create sliding windows"
  meta-description: "How to create sliding windows"
  slug: "/sliding-windows"
  question: "How do I create windowed calculations on time-series data with small advances in time?"
  introduction: "You have time-series records and you want to create windowed aggregations with small increments in time.  You could use hopping-windows, but with small time increments, hopping windows aren't the best solution with small increments in time.  In this tutorial we'll cover how you can achieve efficient windowed aggregations with small advances in time"
  status:
    ksql: disabled
    kstreams: enabled
    kafka: disabled
    c: disabled
    go: disabled
    groovy: disabled
    nodejs: disabled
    python: disabled
    ruby: disabled
    scala: disabled
    swift: disabled
