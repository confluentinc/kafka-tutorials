Kafka Streams applications define their logic in a _processor topology_, which is a graph of stream processors (nodes) and streams (edges). There are two methods for defining these components in your Kafka Streams application, the https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html[Streams DSL] and the https://docs.confluent.io/platform/current/streams/developer-guide/processor-api.html[Processor API]. The Streams DSL provides built-in abstractions for common event stream processing concepts like streams, tables, and transformations, while the Processor API can be used for advanced cases not supported by the DSL.

The Streams DSL is recommended for most use cases and this tutorial will use it to define a basic text processing application. To get started, let's focus on the important bits of Kafka Streams application code, highlighting the DSL usage.

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/creating-first-apache-kafka-streams-application/kstreams/code/tutorial-steps/dev/answer-short.java %}</code></pre>
+++++

In the code above, the `StreamsBuilder` class is used to construct the design of the topology. The DSL API allows you to construct your application by chaining together the desired behaviors using a fluent API.

A typical topology follows a common pattern:

* Consume one or more input streams using the `stream` function which accepts the names of the Kafka topics to consume from along with the deserializers required to decode the data.
* Transform events by chaining together one or more transformations. In our example, we use `mapValues` to convert incoming String events to their upper case value.
* Transformed events are streamed as the output of the topology using the `to` function specifying a destination topic as well as the serializers required to encode the data.

The `peek` function allows you to observe and act on events and they flow through the topology stages. In our example it is used to debug the topology by printing events as they flow through the topology.

Once the topology is defined within the builder, the `buildTopology` function returns an instance of the `Topology` created from `builder.build`. Separating the building of the `Topology` in a function is useful for testing purposes, which we will see in the **Test It** section of the tutorial.

Now go ahead and create the full Java source file at +++<code class="copy-inline">src/main/java/io/confluent/developer/KafkaStreamsApplication.java</code>+++.

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/creating-first-apache-kafka-streams-application/kstreams/code/src/main/java/io/confluent/developer/KafkaStreamsApplication.java %}</code></pre>
+++++
