Now deploy a JDBC sink connector with the code below. Notice no transforms will be necessary to the `eventTime` column because the column is already a supported `TIMESTAMP` type.

+++++
<pre class="snippet"><code class="sql">{% include_raw tutorials/connect-sink-timestamp/ksql/code/tutorial-steps/dev/create-sink-connector.sql %}</code></pre>
+++++

[NOTE]
====
For ksqlDB prior to 0.16.0, the only way to convert `BIGINT` to a `TIMESTAMP` was with https://docs.confluent.io/platform/current/connect/transforms/overview.html[Single Message Transforms (SMTs)]; ksqlDB did not have a `TIMESTAMP` data type.
If you were going this route, you would add something similar to the following to the connector config instead of performing the steps above where we convert the `EVENTTIME` column data type and convert the column to a new timezone.

```
"transforms": "TimestampConverter",
"transforms.TimestampConverter.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
"transforms.TimestampConverter.format": "yyyy-MM-ddTHH:mm:ss.SSS"
"transforms.TimestampConverter.target.type": "Timestamp"
```

Using a SMTs is simple but it has its drawbacks. First, the `TimestampConverter` SMT does not provide users a way to convert timestamp data to other timezones.
Second, the converted timestamp data is not easily consumable by other ksqlDB applications.
Using the ksqlDB functions `FROM_UNIX` and `CONVERT_TZ` allows multiple connectors to use the same transformed data without needing to configure an SMT in each connector.
====

