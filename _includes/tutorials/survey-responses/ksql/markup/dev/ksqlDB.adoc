For this tutorial, we'll be using a respondent dataset to reflect the userbase that's taking the surveys as well as a stream of survey responses where repsondents answer single-question surveys. Kafka Connect is a great tool that can easily stream in datasets from external sources. If we suppose that we're issuing an internal survey, both the respondent and response datasets might exist in a ServiceNow table; use the following template as a guide for setting up your connectors to extract this information and move it to Kafka.
++++
<pre class="snippet expand-default"><code class="sql">
{% include_raw shared/code/ccloud/comment-connector.sql %}
{% include_raw tutorials/survey-responses/ksql/code/tutorial-steps/dev/source.sql %}
</code></pre>
++++
To begin, we first choose to represent our input datasets as either a `TABLE` or a `STREAM`. A `STREAM` represents unbounded, ongoing events while a `TABLE` shows the latest value for a given key. Survey responses are something that may continue to flow into our system--every survey response is valuable in and of itself, and we should care about each one when we do our analysis. Respondent data is something that could change over time. For example, a user could change their user profile which could trigger a new event on the Kafka topic. Because of this, we'll use a `TABLE` for the respondents and a `STREAM` for the survey responses.
++++
<pre class="snippet expand-default"><code class="sql">
{% include_raw tutorials/survey-responses/ksql/code/tutorial-steps/dev/table_stream.sql %}
</code></pre>
++++
As a quick aside: one of the benefits of building a stream processing application is that you can make use of intermediate data streams for different downstream applications. For example, we may want to mask some data from our `SURVEY-RESPONDENTS` stream. Every time a new event flows into the `SURVEY-RESPONDENTS` stream, we can mask the appropriate fields and send the masked data downstream for other applications to tap into.
++++
<pre class="snippet expand-default"><code class="sql">
{% include_raw tutorials/survey-responses/ksql/code/tutorial-steps/dev/mask.sql %}
</code></pre>
++++
With our inputs in ksqlDB, we can start to build up a real time stream processing application. First, we enrich the survey result set with the respondent data--this is done through an inner join. By doing so, we can leverage more of the respondent data in our analysis.
++++
<pre class="snippet expand-default"><code class="sql">
{% include_raw tutorials/survey-responses/ksql/code/tutorial-steps/dev/enrich.sql %}
</code></pre>
++++
This application processes survey responses as they're captured in real time. It aggregates survey responses from a given question and outputs the latest results. We've provided a number of queries to be used for analysis. Since we have access to the respondent data, we can get an idea of the distribution of respondents across teams. Or we can focus on the survey results, themselves.  

Analyzing the events in real time--as opposed to batch--gives the flexibility to see outcomes as they occur or in a windowed fashion depending on the consuming application. A second query has been provided to show how to window over this output result set and only see the final count of survey results after a given window has closed.
++++
<pre class="snippet expand-default"><code class="sql">
{% include_raw tutorials/survey-responses/ksql/code/tutorial-steps/dev/process.sql %}
</code></pre>
++++
