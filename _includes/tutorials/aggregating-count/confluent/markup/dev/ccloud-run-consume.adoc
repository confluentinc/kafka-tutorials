Run the following command to start a Confluent CLI consumer to view consume the events that have been filtered by your application:

```bash
confluent kafka topic consume movie-tickets-sold -b --print-key
```

After the consumer starts, you should see the following messages. Note that for every key (movie), a sequence of output records (count) is emitted. Each record represents an update to the count, which is sent on every movie event specifically because caching is disabled in the code with `StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG` set to `0`. Read more on `https://docs.confluent.io/current/streams/developer-guide/memory-mgmt.html#record-caches-in-the-dsl[Record caches in the DSL]`.

The consumer prompt will hang, waiting for more events to arrive. To continue studying the example, send more events through the input terminal prompt. Otherwise, you can `Control-C` to exit the process.

+++++
<pre class="snippet"><code class="json">{% include_raw tutorials/aggregating-count/kstreams/code/tutorial-steps/dev/outputs/actual-output-events.json %}</code></pre>
+++++

Use `Ctrl-C` to exit.
