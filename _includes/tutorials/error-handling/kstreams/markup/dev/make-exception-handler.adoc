////
In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.
The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.
////

Before you create the Kafka Streams application you'll need to create an instance of a https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/errors/StreamsUncaughtExceptionHandler[StreamsUncaughtExceptionHandler].  For more information you can read https://cwiki.apache.org/confluence/display/KAFKA/KIP-671%3A+Introduce+Kafka+Streams+Specific+Uncaught+Exception+Handler[KIP-671] which introduced the new functionality.

Before we dive into the code let's briefly cover a few points about the `StreamsUncaughtExceptionHander`.

It's an important point to keep in mind that the exception handler will not work for *_all_* exceptions, just those not directly handled by Kafka Streams. An example of an exception that Kafka Streams handles is the https://kafka.apache.org/27/javadoc/org/apache/kafka/common/errors/ProducerFencedException.html[ProducerFencedException] But any exceptions related to your business logic are not dealt with and bubble all the way up to the `StreamThread`, leaving the application no choice but to shut down.  So the `StreamsUncaughtExceptionHandler` gives you a mechanism to take different actions in the case of a thrown exception.

The `StreamsUncaughtExceptionHandler` has one method `handle`, and it returns an `enum` of type https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/errors/StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.html[StreamThreadExceptionResponse] which provides you the opportunity to instruct Kafka Streams how to respond to the exception.  The possible return values are:


* REPLACE_THREAD - Replaces the thread receiving the exception and processing continues with the same number of configured threads.  (Note: this can result in duplicate records depending on the application's processing mode determined by the `PROCESSING_GUARANTEE_CONFIG` value)
* SHUTDOWN_CLIENT - Shut down the individual instance of the Kafka Streams application experiencing the exception.  (This is the previous behavior and the current default behavior if you don't provide a `StreamsUncaughtExceptionHandler`)
* SHUTDOWN_APPLICATION - Shut down all instances of a Kafka Streams application *_with the same application-id_*.  Kafka Streams uses a rebalance to instruct all application instances to shutdown, so even those running on another machine will receive the signal and exit.


For your implementation of the `StreamsUncaughtExceptionHandler`, it will keep track of the number of errors that occur within a given time frame.  If the number of errors exceed the threshold *_within_* the provided timeframe, then the entire application shuts down.  While you could put the exception handling code in a lambda statement, having a separate concrete implementation is better for testing.

Here's the constructor where you provide the max number of failures and the timeframe:

[source, java]
----
public MaxFailuresUncaughtExceptionHandler(final int maxFailures, final long maxTimeIntervalMillis) {
    this.maxFailures = maxFailures;   <1>
    this.maxTimeIntervalMillis = maxTimeIntervalMillis;  <2>
}
----

<1> The max number of failures your application will tolerate within a given timeframe
<2> The max total time allowed for observing the failures

This is probably best understood by taking a look at the core logic:

[source, java]
----
 if (currentFailureCount >= maxFailures) {  <1>
    if (millisBetweenFailure <= maxTimeIntervalMillis) { <2>
        return SHUTDOWN_APPLICATION;
    } else {
        currentFailureCount = 0;  <3>
        previousErrorTime = null;
    }
}
return REPLACE_THREAD;  <4>

----

<1> Checking if the current number of failures equals or exceeds the maximum
<2> Checking if the threshold of max failures occurs within given time window, if yes then shut down.
<3> If you've reached the max number, but the are spread out, reset
<4> The default behavior here is to replace the thread

The idea here is that a couple of errors spread out are ok so processing continues.  But a bunch of errors withing a small window of time could indicate a bigger issue, so it's better to shutdown.  While the code doesn't inspect the type of the exception, that's another valid approach as well.

The above code is just an example of what you could do and definitely not tested in a production setting.  The main point here is while it's a good idea to keep processing with a small number of errors, it's not a good idea to continually replace the thread with sustained errors.  It's better to have some "guard rails" in place to make sure your application is robust, but won't continue on when it shouldn't.

Now create the following file at `src/main/java/io/confluent/developer/MaxFailuresUncaughtExceptionHandler.java`

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/error-handling/kstreams/code/src/main/java/io/confluent/developer/MaxFailuresUncaughtExceptionHandler.java %}</code></pre>
+++++

You'll add the `StreamsUncaughtExceptionHandler` to your Kafka Streams application in the next step.

There's one more step you'll need to take and that is creating a custom exception class `ProcessorException` that your exception handler will process. Create this simple Java class at `src/main/java/io/confluent/developer/ProcessorException.java`

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/error-handling/kstreams/code/src/main/java/io/confluent/developer/ProcessorException.java %}</code></pre>
+++++

NOTE: There is an older, deprecated version of https://kafka.apache.org/28/javadoc/org/apache/kafka/streams/KafkaStreams.html#setUncaughtExceptionHandler-java.lang.Thread.UncaughtExceptionHandler-[KafkaStreams.setUncaughtExceptionHandler] that takes an instance of a `java.lang.Thread.UncaughtExceptionHandler`.  It is advised for users to migrate to use the newer method.
