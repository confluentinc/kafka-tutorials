////
In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.
The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.
////

To complete this tutorial, you'll need to also create an interface for a helper class.


First create the interface at `src/main/java/io/confluent/developer/ConsumerRecordHandler.java`

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/confluent-parallel-consumer-application/kafka/code/src/main/java/io/confluent/developer/ConsumerRecordHandler.java %}</code></pre>
+++++

Using an interface will make it easier to change how you want to work with a `ConsumerRecord` without having to modify all of your existing code.



Next you'll create an implementation of the `ConsumerRecordHandler` interface named `FileWritingRecordHandler`, but before you do that, let's take a peek under the hood to understand how the helper class works.

The `FileWritingRecordHandler` is a simple class that writes values of consumed records to a file. It's worth a quick review of the `process` method:

[source, java]
.FileWritingRecordHandler.process
----
  @Override
  public void process(final ConsumerRecord<String, String> consumerRecord) {
    try {
      Files.write(path, Collections.singletonList(consumerRecord.value()), StandardOpenOption.CREATE, StandardOpenOption.WRITE, StandardOpenOption.APPEND);
    } catch (IOException e) {
      throw new RuntimeException(e);
    }
  }
----
<1> Simply write the record value to a file.

In practice you're certain to do a more realistic workload.

Now go ahead and create the `src/main/java/io/confluent/developer/FileWritingRecordHandler.java` file:

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/confluent-parallel-consumer-application/kafka/code/src/main/java/io/confluent/developer/FileWritingRecordHandler.java %}</code></pre>
+++++
