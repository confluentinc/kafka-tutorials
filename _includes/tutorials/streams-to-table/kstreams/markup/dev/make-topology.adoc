////
In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.
The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.
////



The heart of this tutorial is a simple one liner. You'll take an existing `KStream` object and use the `toTable()` method to covert it into a `KTable`.  This new method (as of Apache Kafka 2.5) allows you to simply convert a https://docs.confluent.io/platform/current/streams/concepts.html#duality-of-streams-and-tables[**record stream** to a **changelog stream**].  In this case you've materialized the `KTable`, so it's available for you to use https://docs.confluent.io/platform/current/streams/developer-guide/interactive-queries.html[Interactive Queries].


++++
<pre class="snippet">
  <code class="java">
    final KStream&lt;String, String&gt; stream = builder.stream(inputTopic, Consumed.with(stringSerde, stringSerde));
    // this line takes the previous KStream and converts it to a KTable
    final KTable&lt;String, String&gt; convertedTable = stream.toTable(Materialized.as("stream-converted-to-table"));
  </code>
</pre>
++++

The rest of this Kafka Streams application simply writes the incoming records back out to a topic.  In the subsequent tutorial steps you'll use a console consumer to observe the differences between a record stream and a changelog stream.

Now go ahead and create the following file at `src/main/java/io/confluent/developer/StreamsToTable.java`.
+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/streams-to-table/kstreams/code/src/main/java/io/confluent/developer/StreamsToTable.java %}</code></pre>
+++++
