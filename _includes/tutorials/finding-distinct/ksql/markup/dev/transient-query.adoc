In the first statement above, we created the query that finds only distinct events, naming it `DETECTED_CLICKS`. We modeled it as a table since the query performs aggregations.

As we're grouping by ip-address, url and timestamp, these columns will become part of the primary key of the table.
Primary key columns are stored in the Kafka message's key. As we'll need them in the value later, we use `AS_VALUE` to copy the columns into the value and set their name. To avoid the value column names clashing with the key columns, we add aliases to rename the key columns.

As it stands, the key of the `DETECTED_CLICKS` table contains the ip-address, url, timestamp columns, and as the table is windowed, the window start time. Wouldn't it be nice if the key was just the IP address?
That's what the next two statements do! The second statement declares a stream on top of the `DETECTED_CLICKS` changelog topic, defining only the value columns we're interested in.
This allows the third statement to set the key of the `DISTINCT_CLICKS` stream to just the IP address. It contains a filter to remove any rows where the ip-address is null to filter out any tombstones from the source topic. Tombstones are sent to the changelog to indicate a row has been deleted from the table; they have a null value, meaning ip-address will be null.

To verify everything is working as expected, run the following query:

+++++
<pre class="snippet"><code class="sql">{% include_raw tutorials/finding-distinct/ksql/code/tutorial-steps/dev/transient-query.sql %}</code></pre>
+++++

The output should look similar to:

+++++
<pre class="snippet"><code class="shell">{% include_raw tutorials/finding-distinct/ksql/code/tutorial-steps/dev/expected-transient-query.log%}</code></pre>
+++++
