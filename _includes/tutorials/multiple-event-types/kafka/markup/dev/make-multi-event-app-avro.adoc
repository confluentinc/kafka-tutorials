Next up in the tutorial is to create the application for the Avro multiple events application.
[source, java]
.KafkaProducer for Avro events
----
public void produceAvroEvents(final Supplier<Producer<String, SpecificRecordBase>> producerSupplier,
                              final String topic,
                              final List<SpecificRecordBase> avroEvents) {

        try (Producer<String, SpecificRecordBase> producer = producerSupplier.get()) {  <1>
           avroEvents.stream()  <2>
                    .map((event -> new ProducerRecord<>(topic, (String) event.get("customer_id"), event))) <3>
                    .forEach(producerRecord -> producer.send(producerRecord,
//Details left out for clarity

//Relevant configurations

 avroProduceConfigs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
 avroProduceConfigs.put(AbstractKafkaSchemaSerDeConfig.AUTO_REGISTER_SCHEMAS, false); <4>
 avroProduceConfigs.put(AbstractKafkaSchemaSerDeConfig.USE_LATEST_VERSION, true) <5>
----

<1> Getting the producer from the supplier
<2> Streaming over the provided collection of Avro events to send
<3> Creating the `ProducerRecord` instance, note the use of map-like access to get the required field for the key
<4> Specifying **_to disable_** automatic schema registration
<5> Setting to use the latest schema

You have a very similar approach with the Avro producer as you did with the Protobuf version, but take a took at the type at annotation one - it's an abstract class, `SpecificRecordBase` that every Avro generated class inherits from.  Since the schema for the Avro multi-event topic uses a `union` at the top level, you don't know the concrete type.  Since you want to use the customer ID as the key you need to access the field in a map-like fashion by using the field name as it exists in the schema.  This is possible because `SpecificRecordBase` implements the `GenericRecord` interface which provides the `get` method for retrieving a field value by name.

But the biggest difference is the configurations you provide to the producer for the Avro serializer, namely disabling automatic schema registration, otherwise it would override the union schema as the latest one.  Additionally since you've set `use.latest.version` to `true` the serializer looks up the latest version, the union schema, and will use that for serialization. https://www.confluent.io/blog/multiple-event-types-in-the-same-kafka-topic/#avro-unions-with-schema-references[This blog post] by Robert Yokota explains this mechanism in detail.


Next we'll move on to creating the Avro consumer.
[source, java]
.KafkaConsumer for multiple events in Avro
----
consumerRecords.forEach(consumerRec -> {
  SpecificRecord avroRecord = consumerRec.value(); <1>
  if (avroRecord instanceof Purchase) {    <2>
      Purchase purchase = (Purchase) avroRecord;  <3>
      eventTracker.add(purchase.getItem());
  } else if (avroRecord instanceof Pageview) {
      Pageview pageView = (Pageview) avroRecord;
      eventTracker.add(pageView.getUrl());

// details left out for clarity
----

<1> Getting the record
<2> Doing an `instanceof` check to determine the type
<3> Casting to the appropriate concrete type

With the Avro consumer you'll need to use the Java `instanceof` operator to determine concrete type for the record.  Notice that here you're using the `SpecificRecord` interface which every Avro generated object implements.  Once you find the correct concrete type you cast the record to that type and extract the required information.

Before you go on to create the application we should mention quickly about the deserialization configurations you need to set

[source, java]
.Configurations needed by the Avro Consumer
----
 avroConsumeConfigs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class); <1>
 avroConsumeConfigs.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true); <2>
----

<1> Specifying to use the Avro deserializer for the Avro consumer
<2> Setting the Avro deserializer to use the specific reader

It should come as no surprise that you need to set the deserializer class to `KafkaAvroDeserializer` for the Avro consumer.  But when working with multiple types, you still need to set the configuration for a specific type.  With Avro, even with the `union` schema, you'll still need to specify to set the `SPECIFIC_AVRO_READER_CONFIG` to `true` to get the concrete types.

Go ahead and create the `src/main/java/io/confluent/developer/MultiEventAvroProduceConsumeApp.java` file:

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/multiple-event-types/kafka/code/src/main/java/io/confluent/developer/MultiEventAvroProduceConsumeApp.java %}</code></pre>
+++++
