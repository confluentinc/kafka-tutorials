////
In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.
The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.
////

Let's say you have a microservice for an e-commerce site, and you track both customer pageviews and purchases.  Since the pageviews could be highly related to any purchase, you'd like to capture the exact order of both of these event types as they occur, so producing the events to the same topic makes sense.

Since you use the customer ID for the key, you are guaranteed to see the exact order of events since both types will live in the same partition.  Even though these two events are similar, you represent them as distinct domain objects as it fits in well with others in your organization that need the same data.


For the Protobuf portion of the tutorial, you'll need to create three protobuf schemas.  Two of the schemas represent the domain objects in the example scenario, and the third schema contains references to the other two schemas.

To get started, create a directory to place the schemas:
++++
<pre class="snippet"><code class="bash">{% include_raw tutorials/multiple-event-types/kafka/code/tutorial-steps/dev/make-proto-dir.sh %}</code></pre>
++++


Then, create this schema file for the purchase domain object at `src/main/proto/purchase.proto`

+++++
<pre class="snippet"><code class="proto">{% include_raw tutorials/multiple-event-types/kafka/code/src/main/proto/purchase.proto %}</code></pre>
+++++

For this tutorial, we won't go into the specifics of Protocol Buffers, but you can read the https://developers.google.com/protocol-buffers/docs/proto3[Proto 3 language guide] and the https://developers.google.com/protocol-buffers/docs/javatutorial[Protobuf Java tutorial] for details beyond the scope of this tutorial.


Next, create the schema for the pageview object at `src/main/proto/pageview.proto`

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/multiple-event-types/kafka/code/src/main/proto/pageview.proto %}</code></pre>
+++++

Now that you have the schemas in place for your two domain objects, you'll create the schema that references the other two.

Go ahead and create the file `src/main/proto/customer-event.proto`, and then we'll review the important parts of it.

[source, proto]
----
syntax = "proto3";

package io.confluent.developer.proto;

import "purchase.proto";
import "pageview.proto";  <1>

option java_outer_classname = "CustomerEventProto";

message CustomerEvent {  <2>

  oneof action {   <3>
    Purchase purchase = 1;
    Pageview page_view = 2;
  }
  string id = 3;
}
----
<1> Importing the other existing proto schema
<2> The outer "container" event
<3> A `oneof` field named `action` which will contain exactly one of the referenced types

This is where the "rubber hits the road" regarding schema references. Here you have the `CustomerEvent` object containing either a `Purchase` or a `Pageview` object in the `action` field.  Instead of nesting schemas for these two objects, we reference existing ones.  In addition to allowing for an effective way to https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#multiple-event-types-in-the-same-topic[combine multiple event types in the same topic] while maintaining https://docs.confluent.io/platform/current/schema-registry/serdes-develop/index.html#subject-name-strategy[the TopicName subject name strategy], by using a reference you get the same benefits that you only have one place you need to go when you need to make schema updates.

Note that, with Protobuf, `oneof` can't be a top-level field.  It has to exist inside a "wrapper" class.  This has implications when producing and consuming, which we will cover when creating the `KafkaProducer` and `KafkaConsumer` for this tutorial.
