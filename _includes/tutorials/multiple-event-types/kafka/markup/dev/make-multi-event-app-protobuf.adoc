////
In this file you describe the Kafka streams topology, and should cover the main points of the tutorial.
The text assumes a method buildTopology exists and constructs the Kafka Streams application.  Feel free to modify the text below to suit your needs.
////

To complete this tutorial, you'll build an application that uses a `KafkaProducer` and  `KafkaConsumer` instance for producing both Avro and Protobuf.  The approach you'll take in this tutorial is not typical of applications you'll build in a production setting. But, by using multiple clients, you can compare how to handle multiple event types for each serializer format.

**To that end, the point of this sample application is this:** you want capture pageview and purchase events in the exact order that they occur and you feel the best option is to have these events produced to the same topic.  Since the customer ID will be the message key, you are guaranteed to get per-customer events in the order that they occur.



Let's go over some of the key parts of the `KafkaMultiEventConsumerApplication` starting with the producer for the Protobuf events.

NOTE: Since this an advanced topic, the tutorial doesn't go into the basics of using a `KafkaProducer`. For more details see the https://creating-first-apache-kafka-producer-application/confluent.html[KafkaProducer tutorial]

[source, java]
.KafkaProducer for Protobuf events
----
public void produceProtobufEvents(final Supplier<Producer<String, CustomerEventProto.CustomerEvent>> producerSupplier,
                                  final String topic,
                                  final List<CustomerEventProto.CustomerEvent> protoCustomerEvents) {

        try (Producer<String, CustomerEventProto.CustomerEvent> producer = producerSupplier.get()) { <1>
            protoCustomerEvents.stream()    <2>
                    .map((event -> new ProducerRecord<>(topic, event.getId(), event))) <3>
                    .forEach(producerRecord -> producer.send(producerRecord, ((metadata, exception)

      //Details left out for clarity

   // Relevant configurations

 protoProduceConfigs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaProtobufSerializer.class);
}
----

<1> Retrieving the producer instance from the `Supplier`
<2> Using a `java.util.stream` to map each event into  a `ProducerRecord` then send them to the broker
<3> Creating the `ProducerRecord` instance.

There are two points to emphasize here.  The first is the type of the producer — it's using `CustomerEventProto.CustomerEvent`.  Since you must use an outer class with Protobuf, the generics on the producer are a concrete type.  As a result, to set the key to be the customer ID you can call the `CustomerEvent#getId` method directly.  Note the use of the `Supplier` to provide the producer this is done to delay the creation until the `Supplier.get()` method is executed.  Using a supplier also makes testing easier by simplifying the process of providing a different implementation.

The second point is that you can use auto-registration feature of Schema Registry with Protobuf and the referenced schemas get registered recursively.

Next, let's move on to the `KafkaConsumer` for the Protobuf application.

[source, java]
.KafkaConsumer for multiple events in Protobuf
----
   consumerRecords.forEach(consumerRec -> {
    CustomerEventProto.CustomerEvent customerEvent = consumerRec.value();
    switch (customerEvent.getActionCase()) { <1>
        case PURCHASE:
            eventTracker.add(customerEvent.getPurchase().getItem()); <2>
            break;
        case PAGE_VIEW:
            eventTracker.add(customerEvent.getPageview().getUrl()); <3>
            break;



// details left out for clarity
----

<1> Using a `switch` statement for the different `enum` types
<2> Adding the purchased item to the event tracker
<3> Adding the pageview link to the event tracker

With Protobuf, when you have a `oneof` field, it generates an `enum` for each message that could be in the field—determining which type to work with can be done by using a `switch` statement.  To retrieve the correct `enum`, you'll use a `get<field-name>Case` method (in this case, `getActionCase` since the `oneof` field is named `action`).

Before you go on to create the application, we should mention the deserialization configurations that you need to set:

[source, java]
.Configurations needed by the Protobuf Consumer
----
 protoConsumeConfigs.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaProtobufDeserializer.class); <1>
 protoConsumeConfigs.put(KafkaProtobufDeserializerConfig.SPECIFIC_PROTOBUF_VALUE_TYPE, CustomerEventProto.CustomerEvent.class); <2>
----

<1> Configurations for the Protobuf consumer to use the Protobuf deserializer
<2> Setting the specific class type for the Protobuf deserializer

It should come as no surprise that you need to set the deserializer class to `KafkaProtobufDeserializer` for the Protobuf consumers.  But, when working with multiple types, you still need to set the configuration for a specific type.  For Protobuf it's straight forward, setting the specific type to the outer class makes sense since the proto deserialization process knows how to handle the embedded types due to the schema.

Now go ahead and create the `src/main/java/io/confluent/developer/MultiEventProtobufProduceConsumeApp.java` file:

+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/multiple-event-types/kafka/code/src/main/java/io/confluent/developer/MultiEventProtobufProduceConsumeApp.java %}</code></pre>
+++++

