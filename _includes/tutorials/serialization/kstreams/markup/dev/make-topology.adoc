Then create the following file at `src/main/java/io/confluent/developer/serialization/SerializationTutorial.java`.
    
+++++
<pre class="snippet"><code class="java">{% include_raw tutorials/serialization/kstreams/code/src/main/java/io/confluent/developer/serialization/SerializationTutorial.java %}</code></pre>
+++++

Let's take a close look at the `buildTopology()` method, which uses the Kafka Streams DSL. This particular topology is pretty simple.
The first thing the method does is create an instance of `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/StreamsBuilder.html[StreamsBuilder]`, which is the helper object that lets us build our topology.
We call the `stream()` method to create a `https://kafka.apache.org/{{ site.ak_javadoc_version }}/javadoc/org/apache/kafka/streams/kstream/KStream.html[KStream]<Long, Movie>` object. Lastly, we call `to()` to send the events to another topic.

All of the work to work to convert the events between JSON and Avro happens through _parameterized serializers_. You see, even though we specified default serializers with `StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG` and `StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG` in Streams Configuration, the Kafka Streams DSL allows us to use a specific serializer / deserializer each time we interact with a topic.

In this case, `Consumed.with()` allows us to consume the events with our custom JSON serde (that we'll be implementing in the next step!), and `Produced.with()` allows us to produce the events back to a topic with Avro.
